{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cleaning_data.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "X71Tqlqv3vHY",
        "fFg6lkQZ4Unf",
        "FRj3ZuOZ4gst"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS906S5hICuM"
      },
      "source": [
        "# 1 - To start with"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFsw9uJNz336"
      },
      "source": [
        "## Drive folder structure\n",
        "Below is the structure of our drive, to clarify the paths variables we used throughout the notebook.\n",
        "\n",
        "```\n",
        ".\n",
        "â”œâ”€â”€ drive \n",
        "â”‚   â”œâ”€â”€ MyDrive             \n",
        "|   |         â”œâ”€â”€ Cleaned_data\n",
        "|   |         |            â”œâ”€â”€ cleaned-quotes-2019.csv.bz2\n",
        "|   |         |            â”œâ”€â”€ ...\n",
        "|   |         â”œâ”€â”€ Processed_datasets\n",
        "|   |         |            â”œâ”€â”€ processed-cleaned-quotes-2019.csv.bz2\n",
        "|   |         |            â”œâ”€â”€ ...\n",
        "|   |         â”œâ”€â”€ Project_datasets\n",
        "|   |         |            â”œâ”€â”€ speakers_attributes.parquet\n",
        "|   |         â”œâ”€â”€ Quotebank\n",
        "|   |         |            â”œâ”€â”€ quotes-2019.json.bz2\n",
        "|   |         |            â”œâ”€â”€ ...\n",
        "|   |         â”œâ”€â”€ gender_extraction\n",
        "|   |         â””â”€â”€ ...\n",
        "â”‚   â””â”€â”€ ...                 \n",
        "â””â”€â”€ ...                \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X71Tqlqv3vHY"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTWZ9wU83x_F"
      },
      "source": [
        "!pip install pandas==1.0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srjbNGSe3-Cc"
      },
      "source": [
        "!pip install pyarrow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFg6lkQZ4Unf"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE3m76UR4QlD"
      },
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import re\n",
        "import itertools\n",
        "import collections\n",
        "import nltk\n",
        "import networkx\n",
        "\n",
        "from glob import glob\n",
        "from urllib.parse import urlparse\n",
        "from dateutil.parser import parse\n",
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRj3ZuOZ4gst"
      },
      "source": [
        "## Mounting Google Drive and getting the python files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-yvegLO4UFG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29a1ff59-ebaf-4287-fcae-89b8f683427b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtTaj2X2NXXY"
      },
      "source": [
        "Load python file for gender extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFLlrIwPIdov"
      },
      "source": [
        " ! cp drive/MyDrive/ADA/gender_extraction.py ."
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFnZJ42ftf51"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DUmAljlyiCh"
      },
      "source": [
        "## Cleaned data\n",
        "Here we can load our processed and cleaned data. We can see that for year 2019, we have roughly 57'000'000 quotes, so this is enough for machine learning tasks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcBvL1tG4jDm"
      },
      "source": [
        "path_to_read_from = '/content/drive/MyDrive/ADA/Processed_datasets'\n",
        "file_name = 'processed-cleaned-quotes-2019.csv.bz2' \n",
        "\n",
        "chunksize = 10000 # = 10k lines\n",
        "\n",
        "# Load dataframe in reader\n",
        "df_reader_cleaned = pd.read_csv(os.path.join(path_to_read_from, file_name), compression ='bz2', chunksize = chunksize)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWZgn050RCS7",
        "outputId": "8e643c76-d665-47a0-bd15-fab9c241f127"
      },
      "source": [
        "valid_quotes_number = 0\n",
        "for chunk in df_reader_cleaned:\n",
        "  valid_quotes_number += chunk.shape[0]\n",
        "\n",
        "print(\"There are {} quotes.\".format(valid_quotes_number))\n"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 56861451 quotes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f38MFqKFnfSb"
      },
      "source": [
        "## Some statistics on speakers attributes\n",
        "For our analysis, we've only selected only binary genders, males and females, as they make up to nearly 100 % of the total speaker pool."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQiCWKU5nez-",
        "outputId": "5483578c-b5fa-4568-d918-864474d3dc1e"
      },
      "source": [
        "path_parquet = '/content/drive/MyDrive/ADA/Project_datasets/speaker_attributes.parquet'\n",
        "\n",
        "# Find all the files absolute path\n",
        "path_parquet_files = path_parquet + '/part*'\n",
        "files = sorted(glob(path_parquet_files))\n",
        "\n",
        "# Find all the files relative path USELESS\n",
        "os.chdir(path_parquet) # might be dangerous because set the current working directory for the entire notebook cells\n",
        "names = glob(\"part*\")\n",
        "\n",
        "nb_males=0\n",
        "nb_females=0\n",
        "\n",
        "for f in files: \n",
        "  df_speakers = pd.read_parquet(f, engine = 'pyarrow',columns = ['gender'])\n",
        "  nb_males = nb_males + df_speakers['gender'].value_counts().values[0]\n",
        "  nb_females = nb_females + df_speakers['gender'].value_counts().values[1]\n",
        "\n",
        "proportion_males = nb_males/(nb_males+nb_females)\n",
        "proportion_females = nb_females/(nb_males+nb_females)\n",
        "\n",
        "print(\"The proportion of female speakers overall is {}% and of male is {}%.\".format(proportion_females*100, proportion_males*100))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The proportion of female speakers overall is 23.711907441661783% and of male is 76.28809255833822%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w2mlVGG9Pb2"
      },
      "source": [
        "# 2 - Cleaning the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i4zxGmAtoTr"
      },
      "source": [
        "## General cleaning functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKRy9kiC_LGF"
      },
      "source": [
        "def clean_chunk(chunk):\n",
        "\n",
        "  \"\"\"Filter out the rows in chunk which:\n",
        "  - have first speaker attribution probability less than 0.5 (empirically)\n",
        "  - have a None speaker or an empty QID\n",
        "  Delete the unnecessary columns for our analysis\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Select the highest probability\n",
        "  chunk['h_probas'] = chunk.apply(lambda p: p['probas'][0][1], axis=1)\n",
        "  # Select the corresponding speaker\n",
        "  chunk['h_probas_speaker'] = chunk.apply(lambda p : p['probas'][0][0], axis=1)\n",
        "  # Select the associated speaker QID\n",
        "  chunk['qids'] = chunk.apply(lambda p : p['qids'][0] if len(p['qids']) >= 1 else p['qids'], axis=1)\n",
        "  \n",
        "\n",
        "# Filter the rows\n",
        "  chunk = chunk.loc[(chunk['speaker'] != 'None') &\n",
        "                    (chunk['h_probas'] > '0.5') &\n",
        "                    (chunk['qids'].astype(str) != '[]')\n",
        "                    ]\n",
        "\n",
        "  return chunk.drop(labels=['probas','h_probas_speaker','quoteID', 'phase'], axis=1)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwpn2zabwKwZ"
      },
      "source": [
        "## Quotation specific cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jR8agc7_Qze"
      },
      "source": [
        "def is_date(string, fuzzy=False):\n",
        "  \"\"\"\n",
        "  Determine whether a given string can be parsed as a date\n",
        "  If it can be, return True, otherwise return False.\n",
        "  @Param : - string : string to be parsed\n",
        "           - fuzzy : boolean allowing fuzzy parsing\n",
        "  @Return : - boolean : whether string can be parsed as a date or not\"\"\"\n",
        "  try:\n",
        "    parse(string, fuzzy = fuzzy)\n",
        "    return True\n",
        "  except ValueError:\n",
        "    return False"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PsL_WfSx85U"
      },
      "source": [
        "def tagfree(string):\n",
        "  \"\"\"\n",
        "  Returns whether a given string is url tag free or not\n",
        "  \"\"\"\n",
        "\n",
        "  # If there is an url in the quotation then it might not be a \"real\" quotation but rather headlines\n",
        "  if (re.search('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', string) != None):\n",
        "    return False\n",
        "    \n",
        "  else:\n",
        "    return True\n",
        "\n",
        "# Uncomment below. The result should be False ! (extracted from real quotes)\n",
        "# tagfree(\"Thomas Tuchel makes me think of Louis van Gaal More Stories < a href = `http://ads.xyz.ng/www/delivery/ck.php?n=a322e25b&cb=787667145' target =' _ blank' > < img src = `http://ads.xyz.ng/www/delivery/avw.php?zoneid=7&cb=565603679&n=a322e25b' border =' 0' alt =\")"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psJq33tOHB0p"
      },
      "source": [
        "def html_tagfree(string):\n",
        "  \"\"\"Takes a string and return wether this string is html tag free or not\"\"\"\n",
        "\n",
        "  if(re.search('<.+?>', string) != None): \n",
        "    return False\n",
        "  else :\n",
        "    return True\n"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMawFwvJDNW8"
      },
      "source": [
        "def clean_quotations(chunk):\n",
        "  \"\"\"\n",
        "  Filters out the incorrect quotations\n",
        "  \"\"\"\n",
        "  # Find all the quotations that are not a date\n",
        "  date_mask = chunk.apply(lambda p: not is_date(p['quotation']), axis = 1)\n",
        "  # Find all the quotations that are tag free\n",
        "  mask = chunk.apply(lambda p: tagfree(p['quotation']), axis = 1)\n",
        "\n",
        "  chunk = chunk.loc[(date_mask & mask) , :]            \n",
        "\n",
        "  return chunk"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjt-U0QrwgA1"
      },
      "source": [
        "## Topic selection functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTZH6Os8xo22"
      },
      "source": [
        "def talks_about(url_list, key_words_list):\n",
        "  \"\"\"Check whether some keywords are inside the urls to retrieve the topics of interest\n",
        "  @Param : - url_list : list of urls (strings)\n",
        "           - key_words_list : list of key words (string)\n",
        "  @Return : True if at least on key word was found in in at least on url, False otherwise \"\"\"\n",
        "\n",
        "  for url in url_list :\n",
        "    if any(key in url for key in key_words_list):\n",
        "      return True\n",
        "  \n",
        "  return False"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKjyyB-wxsBg"
      },
      "source": [
        "def topic_selection(chunk, keywords):\n",
        "  \"\"\"Select the rows which topic is in keywords list:\n",
        "  @Param : - chunk : pandas DatFrame to filter\n",
        "           - Keywords : a list of keywords\"\"\"\n",
        "\n",
        "  topic_relevant_index = chunk.urls.apply(lambda p : talks_about(p, keywords))\n",
        "  \n",
        "  return chunk.drop(chunk[~topic_relevant_index].index)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPyiDHkvBo7v"
      },
      "source": [
        "## Write to file\n",
        "We use several steps to clean our data \n",
        "- First we make general checks on the rows, eg. removing None speakers and thresholding the speaker probability to 0.5, empirically.\n",
        "- Then we remove the quotes that can be parsed as dates and that contain urls\n",
        "- Finally we select the quotes that are topic relevant for us, using a lexical field ðŸŒ³ : ecology, biodiversity, environment, global-warming,ecosystem, sustainability. Climate has been omitted since it can also referred to a mood (climate of anger, etc...)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxwlBTVYBqO_"
      },
      "source": [
        "# QUOTES\n",
        "path_quotes = '/content/drive/MyDrive/ADA/Quotebank'\n",
        "file_quotes = 'quotes-2019.json.bz2' # Leave it zipped !\n",
        " \n",
        "chunksize = 100000 # = 10k\n",
        "# chunk = morceau\n",
        "\n",
        "# Load dataframe in reader\n",
        "df_quotes_reader = pd.read_json(os.path.join(path_quotes, file_quotes), lines=True, compression ='bz2', chunksize = chunksize)\n"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7hmB5ZeBq5E"
      },
      "source": [
        "# Here you need to specify the folder path to write to (ie. you already need to create the folders)\n",
        "path_out = '/content/drive/MyDrive/ADA/Cleaned_data'\n",
        "\n",
        "# Name of the file (depends on the file we read from with df_quotes_reader)\n",
        "cleaned_file_name = 'non_header-cleaned-' + file_quotes[:-8] + 'csv.bz2'\n",
        "header = True\n",
        "\n",
        "for chunk in df_quotes_reader :\n",
        "\n",
        "  # Process chunk\n",
        "  chunk = clean_chunk(chunk) # your cleaning function\n",
        "  chunk = clean_quotations(chunk)\n",
        "  chunk = topic_selection(chunk, ['ecology','biodiversity','environment','global-warming','ecosystem','sustainability'])\n",
        "\n",
        "  # Write to file in memory, avoiding writing the header each time\n",
        "  if header:\n",
        "    chunk.to_csv(path_or_buf = os.path.join(path_out, cleaned_file_name), compression = 'bz2', mode = 'a') # mode a : appends at the end of the same dataframe\n",
        "    header = False\n",
        "  else :\n",
        "    chunk.to_csv(path_or_buf = os.path.join(path_out, cleaned_file_name), compression = 'bz2', mode = 'a', header = False)\n"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FstW8F3kwUVY"
      },
      "source": [
        "#  3 - Enrich data set \n",
        "Enrich data set with speaker gender. Since we need to merge two bug data frames, we enrich our dataset after the first cleaning round. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sEKzKGlwczK"
      },
      "source": [
        "## Gender matching functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKLvqIiVwhh-"
      },
      "source": [
        "def match_and_merge(chunk, gender):\n",
        "  \"\"\"Match a speaker and his/her gender, considering only binary genders\n",
        "  @Param : - chunk : pandas DataFrame\n",
        "           - gender : binary gender DataFrame\n",
        "  @Return :  chunk containing speakers and their gender \"\"\"\n",
        "\n",
        "  # Merge both data frames in an inner joint fashion\n",
        "  chunk = chunk.merge(gender, left_on='qids', right_on='id', how='inner')\n",
        "  \n",
        "  return chunk"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22_ksv2-OOid"
      },
      "source": [
        "## Extract gender from speaker attributes\n",
        "We aim at filtering out all the speakers whose gender is not binary male or female."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R51U7ikoOS86"
      },
      "source": [
        "from gender_extraction import extract_gender\n",
        "\n",
        "extract_gender()"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CduAG2PnxvZl"
      },
      "source": [
        "## Write to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQylTPRdtVkb"
      },
      "source": [
        "# GENDER\n",
        "\n",
        "path_gender = '/content/drive/MyDrive/ADA/Processed_datasets/Gender/speakers-genders.csv'\n",
        "gender = pd.read_csv(path_gender, compression='bz2', index_col = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQfdvR59yAup"
      },
      "source": [
        "# CLEANED QUOTES\n",
        "path_quotes = '/content/drive/MyDrive/ADA/Cleaned_data'\n",
        "file_quotes = 'cleaned-quotes-2019.csv.bz2' # weird name\n",
        "\n",
        " \n",
        "chunksize = 1000 # = 10k\n",
        "# chunk = morceau\n",
        "\n",
        "# Load dataframe in reader\n",
        "df_quotes_reader = pd.read_csv(os.path.join(path_quotes, file_quotes), index_col = 0, compression ='bz2', chunksize = chunksize)"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_PWBL81x2HB"
      },
      "source": [
        " path_out = '/content/drive/MyDrive/ADA/Processed_datasets'\n",
        "\n",
        "# Name of the file (depends on the file we read from with df_reader)\n",
        "cleaned_file_name = 'processed-' + file_quotes\n",
        "header = True\n",
        "\n",
        "for chunk in df_quotes_reader :\n",
        "\n",
        "  # Process chunk\n",
        "  enriched_chunk = match_and_merge(chunk, gender) \n",
        "  \n",
        "  # Write to file in memory\n",
        "  if header :\n",
        "    enriched_chunk.to_csv(path_or_buf = os.path.join(path_out, cleaned_file_name), compression = 'bz2', mode = 'a') # mode a : appends at the end of the same dataframe\n",
        "    header = False\n",
        "  else :\n",
        "    chunk.to_csv(path_or_buf = os.path.join(path_out, cleaned_file_name), compression = 'bz2', mode = 'a', header = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5DCTDAJyd3e"
      },
      "source": [
        "## Some statistical analysis on raw data for dataset of 2019\n",
        "Here we present some filtering we've done, retrieving the quotes we discarded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_NbDUB94xvl"
      },
      "source": [
        "path_to_read_from = '/content/drive/MyDrive/ADA/Quotebank'\n",
        "file_name = 'quotes-2019.json.bz2' \n",
        "\n",
        "chunksize = 1000 # = 10k\n",
        "\n",
        "# Load dataframe in reader\n",
        "df_reader_raw = pd.read_json(os.path.join(path_to_read_from, file_name), lines = True, compression ='bz2', chunksize = chunksize)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29eJ4C-dwsFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94956e78-4774-47ef-82b4-32060eb7920d"
      },
      "source": [
        "## Print quote with urls in it because they cannot be considered as quotes, but rather as kind of headlines\n",
        "\n",
        "for chunk in df_reader_raw :\n",
        "  mask = chunk.apply(lambda p: not tagfree(p['quotation']), axis = 1)\n",
        "  if(chunk.loc[mask,'quotation'].values.size != 0):\n",
        "    print(chunk.loc[mask,'quotation'].values)\n",
        "    break\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"We will do everything we can to ensure he has all the support required to be a huge success.' The Spain based coach will be assisted by Coach Bishir Sadauki who has been with the team for more than a decade. `The Changi Boys' are expected to play some friendly games at home to check the depth of the team before embarking on the pre-season tour next week. Get more stories like this on Twitter AD: To get thousands of free final year project topics and other project materials sorted by subject to help with your research [ click here ] Related Stories MORE FROM AUTHOR Football Fransisca Ordega: Super Falcons can only get better Football Nigeria's Asisat Oshoala set for maiden Women's El Clasico in Spain Football Enyimba Stadium get CAF's approval for Champions League cclash Football FC Barcelona celebrate Samuel Etoâ€™o -- in pictures Football Club rankings: Enyimba retain number one spot in Nigeria Football Club World Cup: Liverpool are beatable -- Gremio president Football Manchester City to swoop for Serie A defender in January Football Super Eagles squad storm Dnipro ahead Ukraine friendly Football Neymar performance surprises Brazil coach Tite More Stories < a href = `http://ads.pista.ng/www/delivery/ck.php?n=a322e25b&cb=787667145' target =' _ blank' > < img src = `http://ads.pista.ng/www/delivery/avw.php?zoneid=7&cb=565603679&n=a322e25b' border =' 0' alt =\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb4JM_AmwwxC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b57c7711-6d17-4c47-ba65-3ce9c7797574"
      },
      "source": [
        "## Print quote with date in it\n",
        "\n",
        "for chunk in df_reader_raw :\n",
        "\n",
        "  date_mask = chunk.apply(lambda p: is_date(p['quotation']), axis = 1)\n",
        "  if(chunk.loc[date_mask,'quotation'].values.size != 0):\n",
        "    print(chunk.loc[date_mask,'quotation'].values)\n",
        "    break\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Nov. 29,' 06.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv0KFiq_I4m9",
        "outputId": "330e2653-66f8-4f17-b408-b953895fa725"
      },
      "source": [
        "## Print quote with html tags in it. We will clean these quotes in a second step.\n",
        "\n",
        "for chunk in df_reader_raw :\n",
        "\n",
        "  mask = chunk.apply(lambda p: not html_tagfree(p['quotation']), axis = 1)\n",
        "  if(chunk.loc[mask,'quotation'].values.size != 0):\n",
        "    print(\"Raw quotation:\\n\",chunk.loc[mask,'quotation'].values[0])\n",
        "    print(\"Cleaned quotation :\\n\", html_cleaning(chunk.loc[mask, 'quotation'].values[0]))\n",
        "    break\n",
        "\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Due to technical issues that cause unexpected crashes and among other reasons, we are pulling < Devotion > off from steam store to have another complete QA check. At the same time we'd like to take this opportunity to ease the heightened pressure in our community result [ ing ] from our previous Art Material Incident, our team would also review our game material once again making sure no other unintended materials was inserted in. Hopefully this would help all audience to focus on the game itself again upon its return.\"]\n",
            "Cleaned quotation : \n",
            " Due to technical issues that cause unexpected crashes and among other reasons we are pulling  Devotion  off from steam store to have another complete QA check At the same time wed like to take this opportunity to ease the heightened pressure in our community result  ing  from our previous Art Material Incident our team would also review our game material once again making sure no other unintended materials was inserted in Hopefully this would help all audience to focus on the game itself again upon its return\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M-5q_4egIug"
      },
      "source": [
        "# 4 - Deeper quotes processing for word frequency and sentiment analysis\n",
        "We need to remove the html tags and hashtags words that are left, and then to lower case all words, and split the quotes into a set of unique words.\n",
        "Word frequency and sentiment analysis seem reasonable here, since we have enough data and \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rs7J8yt0FTB"
      },
      "source": [
        "def html_cleaning(string):\n",
        "  \"\"\"Remove the html and # tags from the string, using a regex\"\"\"\n",
        "  \n",
        "  # from https://www.earthdatascience.org/courses/use-data-open-source-python/intro-to-apis/calculate-tweet-word-frequencies-in-python/\n",
        "  CLEANR = re.compile('([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)')\n",
        "  cleantext = re.sub(CLEANR, '', string)\n",
        "  return cleantext"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbBqiu5uHVVS"
      },
      "source": [
        "def process_quote(string):\n",
        "  \"\"\"Return all the unique words presents in a string in lower case. \"\"\"\n",
        "  return list(set(string.lower().split()))"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LL9XW9oQbbr"
      },
      "source": [
        "# 5 - Methods for word frequency analysis\n",
        "\n",
        "We'll be using nltk library which provides us with tools for sentiment analysis and initial word frequency analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ0txv0e7oM5"
      },
      "source": [
        "path_to_read_from = '/content/drive/MyDrive/ADA/Cleaned_data'\n",
        "file_name = 'cleaned-quotes-2019.csv.bz2' \n",
        "\n",
        "chunksize = 10000 # = 10k lines\n",
        "\n",
        "# Load dataframe in reader\n",
        "df_reader_cleaned = pd.read_csv(os.path.join(path_to_read_from, file_name), index_col=0, compression ='bz2', chunksize = chunksize)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuKoMnOrQgJk",
        "outputId": "9aca5e3a-3946-4066-a74c-da34861ad70b"
      },
      "source": [
        "# Let's provide an example for one chunk\n",
        "for chunk in df_reader_cleaned :\n",
        "  chunk['quotation'] = chunk.apply(lambda p : html_cleaning(p['quotation']), axis = 1)\n",
        "  chunk['quotation'] = chunk.apply(lambda p : process_quote(p['quotation']), axis = 1)\n",
        "\n",
        "  # Retrieve all words\n",
        "  all_words = list(itertools.chain(*chunk.quotation.values))\n",
        "  initial_count = collections.Counter(all_words)\n",
        "  print(initial_count.most_common(15))\n",
        "  print(\"We filter out all the stopwords such as the, a,...\")\n",
        "\n",
        "  # Download stop words\n",
        "  nltk.download('stopwords')\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  # Filter out stop words\n",
        "  quotes_nsw = [[word for word in quote if not word in stop_words]\n",
        "              for quote in chunk.quotation.values]\n",
        "\n",
        "  all_words_nsw = list(itertools.chain(*quotes_nsw))\n",
        "  counts = collections.Counter(all_words_nsw)\n",
        "  print(counts.most_common(15))\n",
        "  \n",
        "  break\n"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 6406), ('to', 5166), ('and', 4782), ('of', 4323), ('a', 3622), ('in', 3103), ('is', 2898), ('that', 2809), ('we', 2719), ('for', 2053), ('are', 1832), ('it', 1772), ('this', 1618), ('have', 1538), ('be', 1471)]\n",
            "We filter out all the stopwords such as the, a,...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[('people', 787), ('climate', 644), ('us', 529), ('change', 525), ('need', 463), ('one', 455), ('environment', 451), ('like', 423), ('new', 415), ('environmental', 408), ('time', 403), ('get', 398), ('going', 398), ('would', 396), ('make', 396)]\n"
          ]
        }
      ]
    }
  ]
}