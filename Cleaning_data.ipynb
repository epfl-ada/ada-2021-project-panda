{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cleaning_data.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "X71Tqlqv3vHY",
        "fFg6lkQZ4Unf",
        "FRj3ZuOZ4gst"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS906S5hICuM"
      },
      "source": [
        "# 1 - To start with"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFsw9uJNz336"
      },
      "source": [
        "## Drive folder structure\n",
        "Below is the structure of our drive, to clarify the paths variables we used throughout the notebook.\n",
        "\n",
        "```\n",
        ".\n",
        "├── drive \n",
        "│   ├── MyDrive             \n",
        "|   |         ├── Cleaned_data\n",
        "|   |         |            ├── cleaned-quotes-2019.csv.bz2\n",
        "|   |         |            ├── ...\n",
        "|   |         ├── Processed_datasets\n",
        "|   |         |            ├── processed-cleaned-quotes-2019.csv.bz2\n",
        "|   |         |            ├── ...\n",
        "|   |         ├── Project_datasets\n",
        "|   |         |            ├── speakers_attributes.parquet\n",
        "|   |         ├── Quotebank\n",
        "|   |         |            ├── quotes-2019.json.bz2\n",
        "|   |         |            ├── ...\n",
        "|   |         ├── gender_extraction\n",
        "|   |         └── ...\n",
        "│   └── ...                 \n",
        "└── ...                \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X71Tqlqv3vHY"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTWZ9wU83x_F"
      },
      "source": [
        "!pip install pandas==1.0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srjbNGSe3-Cc"
      },
      "source": [
        "!pip install pyarrow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFg6lkQZ4Unf"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE3m76UR4QlD"
      },
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import re\n",
        "import itertools\n",
        "import collections\n",
        "import nltk\n",
        "import networkx\n",
        "\n",
        "from glob import glob\n",
        "from urllib.parse import urlparse\n",
        "from dateutil.parser import parse\n",
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRj3ZuOZ4gst"
      },
      "source": [
        "## Mounting Google Drive and getting the python files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-yvegLO4UFG"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtTaj2X2NXXY"
      },
      "source": [
        "Load python file for gender extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFLlrIwPIdov"
      },
      "source": [
        " ! cp drive/MyDrive/ADA/gender_extraction.py ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFnZJ42ftf51"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DUmAljlyiCh"
      },
      "source": [
        "## Cleaned data\n",
        "Here we can load our processed and cleaned data. We can see that for year 2019, we have roughly 57'000'000 quotes, so this is enough for machine learning tasks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcBvL1tG4jDm"
      },
      "source": [
        "path_to_read_from = '/content/drive/MyDrive/ADA/Processed_datasets'\n",
        "file_name = 'processed-cleaned-quotes-2019.csv.bz2' \n",
        "\n",
        "chunksize = 10000 # = 10k lines\n",
        "\n",
        "# Load dataframe in reader\n",
        "df_reader_cleaned = pd.read_csv(os.path.join(path_to_read_from, file_name), compression ='bz2', chunksize = chunksize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWZgn050RCS7"
      },
      "source": [
        "valid_quotes_number = 0\n",
        "for chunk in df_reader_cleaned:\n",
        "  valid_quotes_number += chunk.shape[0]\n",
        "\n",
        "print(\"There are {} quotes.\".format(valid_quotes_number))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f38MFqKFnfSb"
      },
      "source": [
        "## Some statistics on speakers attributes\n",
        "For our analysis, we've only selected only binary genders, males and females, as they make up to nearly 100 % of the total speaker pool."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQiCWKU5nez-"
      },
      "source": [
        "path_parquet = '/content/drive/MyDrive/ADA/Project_datasets/speaker_attributes.parquet'\n",
        "\n",
        "# Find all the files absolute path\n",
        "path_parquet_files = path_parquet + '/part*'\n",
        "files = sorted(glob(path_parquet_files))\n",
        "\n",
        "# Find all the files relative path USELESS\n",
        "os.chdir(path_parquet) # might be dangerous because set the current working directory for the entire notebook cells\n",
        "names = glob(\"part*\")\n",
        "\n",
        "nb_males=0\n",
        "nb_females=0\n",
        "\n",
        "for f in files: \n",
        "  df_speakers = pd.read_parquet(f, engine = 'pyarrow',columns = ['gender'])\n",
        "  nb_males = nb_males + df_speakers['gender'].value_counts().values[0]\n",
        "  nb_females = nb_females + df_speakers['gender'].value_counts().values[1]\n",
        "\n",
        "proportion_males = nb_males/(nb_males+nb_females)\n",
        "proportion_females = nb_females/(nb_males+nb_females)\n",
        "\n",
        "print(\"The proportion of female speakers overall is {}% and of male is {}%.\".format(proportion_females*100, proportion_males*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w2mlVGG9Pb2"
      },
      "source": [
        "# 2 - Cleaning the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i4zxGmAtoTr"
      },
      "source": [
        "## General cleaning functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKRy9kiC_LGF"
      },
      "source": [
        "def clean_chunk(chunk):\n",
        "\n",
        "  \"\"\"Filter out the rows in chunk which:\n",
        "  - have first speaker attribution probability less than 0.5 (empirically)\n",
        "  - have a None speaker or an empty QID\n",
        "  Delete the unnecessary columns for our analysis\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Select the highest probability\n",
        "  chunk['h_probas'] = chunk.apply(lambda p: p['probas'][0][1], axis=1)\n",
        "  # Select the corresponding speaker\n",
        "  chunk['h_probas_speaker'] = chunk.apply(lambda p : p['probas'][0][0], axis=1)\n",
        "  # Select the associated speaker QID\n",
        "  chunk['qids'] = chunk.apply(lambda p : p['qids'][0] if len(p['qids']) >= 1 else p['qids'], axis=1)\n",
        "  \n",
        "\n",
        "# Filter the rows\n",
        "  chunk = chunk.loc[(chunk['speaker'] != 'None') &\n",
        "                    (chunk['h_probas'] > '0.5') &\n",
        "                    (chunk['qids'].astype(str) != '[]')\n",
        "                    ]\n",
        "\n",
        "  return chunk.drop(labels=['probas','h_probas_speaker','quoteID', 'phase'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwpn2zabwKwZ"
      },
      "source": [
        "## Quotation specific cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jR8agc7_Qze"
      },
      "source": [
        "def is_date(string, fuzzy=False):\n",
        "  \"\"\"\n",
        "  Determine whether a given string can be parsed as a date\n",
        "  If it can be, return True, otherwise return False.\n",
        "  @Param : - string : string to be parsed\n",
        "           - fuzzy : boolean allowing fuzzy parsing\n",
        "  @Return : - boolean : whether string can be parsed as a date or not\"\"\"\n",
        "  try:\n",
        "    parse(string, fuzzy = fuzzy)\n",
        "    return True\n",
        "  except ValueError:\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PsL_WfSx85U"
      },
      "source": [
        "def tagfree(string):\n",
        "  \"\"\"\n",
        "  Returns whether a given string is url tag free or not\n",
        "  \"\"\"\n",
        "\n",
        "  # If there is an url in the quotation then it might not be a \"real\" quotation but rather headlines\n",
        "  if (re.search('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', string) != None):\n",
        "    return False\n",
        "    \n",
        "  else:\n",
        "    return True\n",
        "\n",
        "# Uncomment below. The result should be False ! (extracted from real quotes)\n",
        "# tagfree(\"Thomas Tuchel makes me think of Louis van Gaal More Stories < a href = `http://ads.xyz.ng/www/delivery/ck.php?n=a322e25b&cb=787667145' target =' _ blank' > < img src = `http://ads.xyz.ng/www/delivery/avw.php?zoneid=7&cb=565603679&n=a322e25b' border =' 0' alt =\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psJq33tOHB0p"
      },
      "source": [
        "def html_tagfree(string):\n",
        "  \"\"\"Takes a string and return wether this string is html tag free or not\"\"\"\n",
        "\n",
        "  if(re.search('<.+?>', string) != None): \n",
        "    return False\n",
        "  else :\n",
        "    return True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMawFwvJDNW8"
      },
      "source": [
        "def clean_quotations(chunk):\n",
        "  \"\"\"\n",
        "  Filters out the incorrect quotations\n",
        "  \"\"\"\n",
        "  # Find all the quotations that are not a date\n",
        "  date_mask = chunk.apply(lambda p: not is_date(p['quotation']), axis = 1)\n",
        "  # Find all the quotations that are tag free\n",
        "  mask = chunk.apply(lambda p: tagfree(p['quotation']), axis = 1)\n",
        "\n",
        "  chunk = chunk.loc[(date_mask & mask) , :]            \n",
        "\n",
        "  return chunk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjt-U0QrwgA1"
      },
      "source": [
        "## Topic selection functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTZH6Os8xo22"
      },
      "source": [
        "def talks_about(url_list, key_words_list):\n",
        "  \"\"\"Check whether some keywords are inside the urls to retrieve the topics of interest\n",
        "  @Param : - url_list : list of urls (strings)\n",
        "           - key_words_list : list of key words (string)\n",
        "  @Return : True if at least on key word was found in in at least on url, False otherwise \"\"\"\n",
        "\n",
        "  for url in url_list :\n",
        "    if any(key in url for key in key_words_list):\n",
        "      return True\n",
        "  \n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKjyyB-wxsBg"
      },
      "source": [
        "def topic_selection(chunk, keywords):\n",
        "  \"\"\"Select the rows which topic is in keywords list:\n",
        "  @Param : - chunk : pandas DatFrame to filter\n",
        "           - Keywords : a list of keywords\"\"\"\n",
        "\n",
        "  topic_relevant_index = chunk.urls.apply(lambda p : talks_about(p, keywords))\n",
        "  \n",
        "  return chunk.drop(chunk[~topic_relevant_index].index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPyiDHkvBo7v"
      },
      "source": [
        "## Write to file\n",
        "We use several steps to clean our data \n",
        "- First we make general checks on the rows, eg. removing None speakers and thresholding the speaker probability to 0.5, empirically.\n",
        "- Then we remove the quotes that can be parsed as dates and that contain urls\n",
        "- Finally we select the quotes that are topic relevant for us, using a lexical field 🌳 : ecology, biodiversity, environment, global-warming,ecosystem, sustainability. Climate has been omitted since it can also referred to a mood (climate of anger, etc...)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxwlBTVYBqO_"
      },
      "source": [
        "# QUOTES\n",
        "path_quotes = '/content/drive/MyDrive/ADA/Quotebank'\n",
        "file_quotes = 'quotes-2019.json.bz2' # Leave it zipped !\n",
        " \n",
        "chunksize = 100000 # = 10k\n",
        "# chunk = morceau\n",
        "\n",
        "# Load dataframe in reader\n",
        "df_quotes_reader = pd.read_json(os.path.join(path_quotes, file_quotes), lines=True, compression ='bz2', chunksize = chunksize)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7hmB5ZeBq5E"
      },
      "source": [
        "# Here you need to specify the folder path to write to (ie. you already need to create the folders)\n",
        "path_out = '/content/drive/MyDrive/ADA/Cleaned_data'\n",
        "\n",
        "# Name of the file (depends on the file we read from with df_quotes_reader)\n",
        "cleaned_file_name = 'non_header-cleaned-' + file_quotes[:-8] + 'csv.bz2'\n",
        "header = True\n",
        "\n",
        "for chunk in df_quotes_reader :\n",
        "\n",
        "  # Process chunk\n",
        "  chunk = clean_chunk(chunk) # your cleaning function\n",
        "  chunk = clean_quotations(chunk)\n",
        "  chunk = topic_selection(chunk, ['ecology','biodiversity','environment','global-warming','ecosystem','sustainability'])\n",
        "\n",
        "  # Write to file in memory, avoiding writing the header each time\n",
        "  if header:\n",
        "    chunk.to_csv(path_or_buf = os.path.join(path_out, cleaned_file_name), compression = 'bz2', mode = 'a') # mode a : appends at the end of the same dataframe\n",
        "    header = False\n",
        "  else :\n",
        "    chunk.to_csv(path_or_buf = os.path.join(path_out, cleaned_file_name), compression = 'bz2', mode = 'a', header = False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FstW8F3kwUVY"
      },
      "source": [
        "#  3 - Enrich data set \n",
        "Enrich data set with speaker gender. Since we need to merge two bug data frames, we enrich our dataset after the first cleaning round. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sEKzKGlwczK"
      },
      "source": [
        "## Gender matching functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKLvqIiVwhh-"
      },
      "source": [
        "def match_and_merge(chunk, gender):\n",
        "  \"\"\"Match a speaker and his/her gender, considering only binary genders\n",
        "  @Param : - chunk : pandas DataFrame\n",
        "           - gender : binary gender DataFrame\n",
        "  @Return :  chunk containing speakers and their gender \"\"\"\n",
        "\n",
        "  # Merge both data frames in an inner joint fashion\n",
        "  chunk = chunk.merge(gender, left_on='qids', right_on='id', how='inner')\n",
        "  \n",
        "  return chunk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22_ksv2-OOid"
      },
      "source": [
        "## Extract gender from speaker attributes\n",
        "We aim at filtering out all the speakers whose gender is not binary male or female."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R51U7ikoOS86"
      },
      "source": [
        "from gender_extraction import extract_gender\n",
        "\n",
        "extract_gender()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CduAG2PnxvZl"
      },
      "source": [
        "## Write to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQylTPRdtVkb"
      },
      "source": [
        "# GENDER\n",
        "\n",
        "path_gender = '/content/drive/MyDrive/ADA/Processed_datasets/Gender/speakers-genders.csv'\n",
        "gender = pd.read_csv(path_gender, compression='bz2', index_col = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQfdvR59yAup"
      },
      "source": [
        "# CLEANED QUOTES\n",
        "path_quotes = '/content/drive/MyDrive/ADA/Cleaned_data'\n",
        "file_quotes = 'cleaned-quotes-2019.csv.bz2' # weird name\n",
        "\n",
        " \n",
        "chunksize = 1000 # = 10k\n",
        "# chunk = morceau\n",
        "\n",
        "# Load dataframe in reader\n",
        "df_quotes_reader = pd.read_csv(os.path.join(path_quotes, file_quotes), index_col = 0, compression ='bz2', chunksize = chunksize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_PWBL81x2HB"
      },
      "source": [
        " path_out = '/content/drive/MyDrive/ADA/Processed_datasets'\n",
        "\n",
        "# Name of the file (depends on the file we read from with df_reader)\n",
        "cleaned_file_name = 'processed-' + file_quotes\n",
        "header = True\n",
        "\n",
        "for chunk in df_quotes_reader :\n",
        "\n",
        "  # Process chunk\n",
        "  enriched_chunk = match_and_merge(chunk, gender) \n",
        "  \n",
        "  # Write to file in memory\n",
        "  if header :\n",
        "    enriched_chunk.to_csv(path_or_buf = os.path.join(path_out, cleaned_file_name), compression = 'bz2', mode = 'a') # mode a : appends at the end of the same dataframe\n",
        "    header = False\n",
        "  else :\n",
        "    chunk.to_csv(path_or_buf = os.path.join(path_out, cleaned_file_name), compression = 'bz2', mode = 'a', header = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5DCTDAJyd3e"
      },
      "source": [
        "## Some statistical analysis on raw data for dataset of 2019\n",
        "Here we present some filtering we've done, retrieving the quotes we discarded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_NbDUB94xvl"
      },
      "source": [
        "path_to_read_from = '/content/drive/MyDrive/ADA/Quotebank'\n",
        "file_name = 'quotes-2019.json.bz2' \n",
        "\n",
        "chunksize = 1000 # = 10k\n",
        "\n",
        "# Load dataframe in reader\n",
        "df_reader_raw = pd.read_json(os.path.join(path_to_read_from, file_name), lines = True, compression ='bz2', chunksize = chunksize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29eJ4C-dwsFg"
      },
      "source": [
        "## Print quote with urls in it because they cannot be considered as quotes, but rather as kind of headlines\n",
        "\n",
        "for chunk in df_reader_raw :\n",
        "  mask = chunk.apply(lambda p: not tagfree(p['quotation']), axis = 1)\n",
        "  if(chunk.loc[mask,'quotation'].values.size != 0):\n",
        "    print(chunk.loc[mask,'quotation'].values)\n",
        "    break\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb4JM_AmwwxC"
      },
      "source": [
        "## Print quote with date in it\n",
        "\n",
        "for chunk in df_reader_raw :\n",
        "\n",
        "  date_mask = chunk.apply(lambda p: is_date(p['quotation']), axis = 1)\n",
        "  if(chunk.loc[date_mask,'quotation'].values.size != 0):\n",
        "    print(chunk.loc[date_mask,'quotation'].values)\n",
        "    break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv0KFiq_I4m9"
      },
      "source": [
        "## Print quote with html tags in it. We will clean these quotes in a second step.\n",
        "\n",
        "for chunk in df_reader_raw :\n",
        "\n",
        "  mask = chunk.apply(lambda p: not html_tagfree(p['quotation']), axis = 1)\n",
        "  if(chunk.loc[mask,'quotation'].values.size != 0):\n",
        "    print(\"Raw quotation:\\n\",chunk.loc[mask,'quotation'].values[0])\n",
        "    print(\"Cleaned quotation :\\n\", html_cleaning(chunk.loc[mask, 'quotation'].values[0]))\n",
        "    break\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M-5q_4egIug"
      },
      "source": [
        "# 4 - Deeper quotes processing for word frequency and sentiment analysis\n",
        "We need to remove the html tags and hashtags words that are left, and then to lower case all words, and split the quotes into a set of unique words.\n",
        "Word frequency and sentiment analysis seem reasonable here, since we have enough data and \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rs7J8yt0FTB"
      },
      "source": [
        "def html_cleaning(string):\n",
        "  \"\"\"Remove the html and # tags from the string, using a regex\"\"\"\n",
        "  \n",
        "  # from https://www.earthdatascience.org/courses/use-data-open-source-python/intro-to-apis/calculate-tweet-word-frequencies-in-python/\n",
        "  CLEANR = re.compile('([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)')\n",
        "  cleantext = re.sub(CLEANR, '', string)\n",
        "  return cleantext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbBqiu5uHVVS"
      },
      "source": [
        "def process_quote(string):\n",
        "  \"\"\"Return all the unique words presents in a string in lower case. \"\"\"\n",
        "  return list(set(string.lower().split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LL9XW9oQbbr"
      },
      "source": [
        "# 5 - Methods for word frequency analysis\n",
        "\n",
        "We'll be using nltk library which provides us with tools for sentiment analysis and initial word frequency analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ0txv0e7oM5"
      },
      "source": [
        "path_to_read_from = '/content/drive/MyDrive/ADA/Cleaned_data'\n",
        "file_name = 'cleaned-quotes-2019.csv.bz2' \n",
        "\n",
        "chunksize = 10000 # = 10k lines\n",
        "\n",
        "# Load dataframe in reader\n",
        "df_reader_cleaned = pd.read_csv(os.path.join(path_to_read_from, file_name), index_col=0, compression ='bz2', chunksize = chunksize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuKoMnOrQgJk"
      },
      "source": [
        "# Let's provide an example for one chunk\n",
        "for chunk in df_reader_cleaned :\n",
        "  chunk['quotation'] = chunk.apply(lambda p : html_cleaning(p['quotation']), axis = 1)\n",
        "  chunk['quotation'] = chunk.apply(lambda p : process_quote(p['quotation']), axis = 1)\n",
        "\n",
        "  # Retrieve all words\n",
        "  all_words = list(itertools.chain(*chunk.quotation.values))\n",
        "  initial_count = collections.Counter(all_words)\n",
        "  print(initial_count.most_common(15))\n",
        "  print(\"We filter out all the stopwords such as the, a,...\")\n",
        "\n",
        "  # Download stop words\n",
        "  nltk.download('stopwords')\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  # Filter out stop words\n",
        "  quotes_nsw = [[word for word in quote if not word in stop_words]\n",
        "              for quote in chunk.quotation.values]\n",
        "\n",
        "  all_words_nsw = list(itertools.chain(*quotes_nsw))\n",
        "  counts = collections.Counter(all_words_nsw)\n",
        "  print(counts.most_common(15))\n",
        "  \n",
        "  break\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}